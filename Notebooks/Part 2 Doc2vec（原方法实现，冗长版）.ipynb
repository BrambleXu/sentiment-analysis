{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Doc2vec（原方法实现，冗长版）\n",
    "\n",
    "# 资料介绍\n",
    "\n",
    "[A gentle introduction to Doc2Vec](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)。\n",
    "\n",
    "[word2vec-sentiments](https://github.com/linanqiu/word2vec-sentiments)\n",
    "\n",
    "上面的文章也说到了，doc2vec是一种非监督的方法，适合用于情感分析。类似于word2vec，经过训练后，每一段都能用一个vector来代表。所以我们可以用这些vector代替特征值，来训练模型。\n",
    "\n",
    "不过这里有个问题，输入doc2vec中的数据，是LabeledSentence objects的迭代器。每一个LabeledSentence object都代表一句话，这句话由两部分组成：由word组成的list，和由labels组成的list。不过有label的话，这不就相当于监督式学习了？这样的话数据集里的unlabeledTrainData不就用不到了？\n",
    "\n",
    "# 方法1：只训练有标签的数据集\n",
    "\n",
    "这部分就用[word2vec-sentiments](https://github.com/linanqiu/word2vec-sentiments)的方法吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gensim modules\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# random\n",
    "from random import shuffle\n",
    "\n",
    "# preprocess packages\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords # import the stop word list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Input Format\n",
    "\n",
    "- `train-neg.txt`: 12500 negative movie reviews from the training data\n",
    "- `train-pos.txt`: 12500 positive movie reviews from the training data\n",
    "- `train-unsup.txt`: 50000 Unlabelled movie reviews\n",
    "\n",
    "下面是两个评论样本，要先对原文进行处理，比如全部小写，去标点。每个样本就是一行，每一行用回车隔开，这样才能被识别。\n",
    "\n",
    "```\n",
    "once again mr costner has dragged out a movie for far longer than necessary aside from the terrific sea rescue sequences of which there are very few i just did not care about any of the characters most of us have ghosts in the closet and costner s character are realized early on and then forgotten until much later by which time i did not care the character we should really care about is a very cocky overconfident ashton kutcher the problem is he comes off as kid who thinks he s better than anyone else around him and shows no signs of a cluttered closet his only obstacle appears to be winning over costner finally when we are well past the half way point of this stinker costner tells us all about kutcher s ghosts we are told why kutcher is driven to be the best with no prior inkling or foreshadowing no magic here it was all i could do to keep from turning it off an hour in\n",
    "this is an example of why the majority of action films are the same generic and boring there s really nothing worth watching here a complete waste of the then barely tapped talents of ice t and ice cube who ve each proven many times over that they are capable of acting and acting well don t bother with this one go see new jack city ricochet or watch new york undercover for ice t or boyz n the hood higher learning or friday for ice cube and see the real deal ice t s horribly cliched dialogue alone makes this film grate at the teeth and i m still wondering what the heck bill paxton was doing in this film and why the heck does he always play the exact same character from aliens onward every film i ve seen with bill paxton has him playing the exact same irritating character and at least in aliens his character died which made it somewhat gratifying overall this is second rate action trash there are countless better films to see and if you really want to see this one watch judgement night which is practically a carbon copy but has better acting and a better script the only thing that made this at all worth watching was a decent hand on the camera the cinematography was almost refreshing which comes close to making up for the horrible film itself but not quite\n",
    "```\n",
    "\n",
    "处理方法，作者只进行了小写化和去标点，我打算还打算去html和stop words，直接用之前写的函数就行了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../Sentiment/data/labeledTrainData.tsv\", header=0, \n",
    "                         delimiter='\\t', quoting=3, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review, \"lxml\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie reviews...\n",
      "\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning and parsing the training set movie reviews...\\n\")\n",
    "\n",
    "# number of reviews\n",
    "num_reviews = train['review'].size\n",
    "\n",
    "# initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "\n",
    "for i in range( 0, num_reviews):\n",
    "    # If the index is evenly divisible by 5000, print a message\n",
    "    if( (i+1)%5000 == 0 ):\n",
    "        print(\"Review %d of %d\\n\" % (i+1, num_reviews))                                                                  \n",
    "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对test data也做同样的处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test set movie reviews...\n",
      "Review 5000 of 25000\n",
      "\n",
      "Review 10000 of 25000\n",
      "\n",
      "Review 15000 of 25000\n",
      "\n",
      "Review 20000 of 25000\n",
      "\n",
      "Review 25000 of 25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Data\n",
    "test = pd.read_csv(\"../Sentiment/data/testData.tsv\", header = 0, delimiter = \"\\t\", quoting = 3)\n",
    "num_reviews = len(test[\"review\"])\n",
    "clean_test_reviews = []\n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\")\n",
    "for i in range( 0, num_reviews):\n",
    "    if( (i+1)%5000 == 0 ):\n",
    "        print(\"Review %d of %d\\n\" % (i+1, num_reviews))                                                                  \n",
    "    clean_review = review_to_words(test[\"review\"][i])\n",
    "    clean_test_reviews.append(clean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对unlabeled data也做同样处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test set movie reviews...\n",
      "Review 5000 of 50000\n",
      "\n",
      "Review 10000 of 50000\n",
      "\n",
      "Review 15000 of 50000\n",
      "\n",
      "Review 20000 of 50000\n",
      "\n",
      "Review 25000 of 50000\n",
      "\n",
      "Review 30000 of 50000\n",
      "\n",
      "Review 35000 of 50000\n",
      "\n",
      "Review 40000 of 50000\n",
      "\n",
      "Review 45000 of 50000\n",
      "\n",
      "Review 50000 of 50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unlabeled Train Data \n",
    "unlabeled_reviews = pd.read_csv(\"../Sentiment/data/unlabeledTrainData.tsv\", header = 0, delimiter = \"\\t\", quoting = 3)\n",
    "num_reviews = len(unlabeled_reviews[\"review\"])\n",
    "clean_unlabeled_reviews = []\n",
    "\n",
    "print(\"Cleaning and parsing the test set movie reviews...\")\n",
    "for i in range( 0, num_reviews):\n",
    "    if( (i+1)%5000 == 0 ):\n",
    "        print(\"Review %d of %d\\n\" % (i+1, num_reviews))                                                                  \n",
    "    clean_review = review_to_words(unlabeled_reviews[\"review\"][i])\n",
    "    clean_unlabeled_reviews.append(clean_review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim的Doc2Vec应用于训练要求每一篇文章/句子有一个唯一标识的label。这里gensim有TaggedDocument和LabeledSentence两个方法。不过通过调查，发现TaggedDocument更新一些，最终会取代LabeledSentence，所以这里我们使用TaggedDocument。\n",
    "\n",
    "我们使用Gensim自带的TaggedDocument方法. 标识的格式为\"TRAIN_i\"和\"TEST_i\"，其中i为序号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_reviews(reviews, prefix):\n",
    "    tagged = []\n",
    "    for i, review in enumerate(reviews):\n",
    "        tagged.append(TaggedDocument(words=review.split(), tags=[prefix + '_%s' % i]))\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tagged = tag_reviews(clean_train_reviews, 'TRAIN')\n",
    "test_tagged = tag_reviews(clean_test_reviews, 'TEST')\n",
    "unlabeled_train_tagged = tag_reviews(clean_unlabeled_reviews, 'UNTRAIN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，到此为止需要输入的数据就准备好了。下面查看一些我们得到的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mj', 'music', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scene', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'gave', 'subject', 'hmmm', 'well', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter'], tags=['TRAIN_0'])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['accurate', 'depiction', 'small', 'time', 'mob', 'life', 'filmed', 'new', 'jersey', 'story', 'characters', 'script', 'believable', 'acting', 'drops', 'ball', 'still', 'worth', 'watching', 'especially', 'strong', 'images', 'still', 'even', 'though', 'first', 'viewed', 'years', 'ago', 'young', 'hood', 'steps', 'starts', 'bigger', 'things', 'tries', 'things', 'keep', 'going', 'wrong', 'leading', 'local', 'boss', 'suspect', 'end', 'skimmed', 'good', 'place', 'enjoy', 'health', 'life', 'film', 'introduced', 'joe', 'pesce', 'martin', 'scorsese', 'also', 'present', 'perennial', 'screen', 'wise', 'guy', 'frank', 'vincent', 'strong', 'characterizations', 'visuals', 'sound', 'muddled', 'much', 'acting', 'amateurish', 'great', 'story'], tags=['TEST_4'])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tagged[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['well', 'made', 'gritty', 'science', 'fiction', 'movie', 'could', 'lost', 'among', 'hundreds', 'similar', 'movies', 'several', 'strong', 'points', 'keep', 'near', 'top', 'one', 'writing', 'directing', 'solid', 'manages', 'part', 'avoid', 'many', 'sci', 'fi', 'cliches', 'though', 'good', 'job', 'keeping', 'suspense', 'landscape', 'look', 'movie', 'appeal', 'sci', 'fi', 'fans', 'looking', 'masterpiece', 'looking', 'good', 'old', 'fashioned', 'post', 'apoc', 'gritty', 'future', 'space', 'sci', 'fi', 'good', 'suspense', 'special', 'effects', 'movie', 'thoroughly', 'enjoyable', 'good', 'ending'], tags=['UNTRAIN_8'])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train_tagged[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是使用LabeledSentence实现的例子，效果一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labelizeReviews(reviews, label_type):\n",
    "    labelized = []\n",
    "    for i, review in enumerate(reviews):\n",
    "        label = '%s_%s'%(label_type, i)\n",
    "        labelized.append(LabeledSentence(review.split(), [label]))\n",
    "    return labelized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tagged = labelizeReviews(clean_train_reviews, 'TRAIN')\n",
    "test_tagged = labelizeReviews(clean_test_reviews, 'TEST')\n",
    "unlabeled_train_tagged = labelizeReviews(clean_unlabeled_reviews, 'UNTRAIN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "\n",
    "## Building the Vocabulary Table\n",
    "\n",
    "这里使用build_vocab，可以把所有的数据全部输入，包括test数据集的也可以。只是用于建立词典而已，不会有因为训练集的存在而造成数据泄露。\n",
    "\n",
    "doc2vec与word2vec一样，也有两种模型，一种是和CBOW相似的PV-DM model，一种是和skip-gram相似的PV-DBOW。根据文章的讲解，PV-DM效果更好，并经常做到state-of-art，这里我们就先拿PV-DM做个例子。\n",
    "\n",
    "dm (int {1,0}) – Defines the training algorithm. If dm=1, ‘distributed memory’ (PV-DM) is used. Otherwise, distributed bag of words (PV-DBOW) is employed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(min_count=1, window=10, size=100, sample=1e-3, negative=5, dm=0, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-23280f83b45c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_dbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_tagged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tagged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_train_tagged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/xu/anaconda/envs/py35/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \"\"\"\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# initial survey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build tables & arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/xu/anaconda/envs/py35/lib/python3.5/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, documents, progress_per, trim_rule, update)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m                     logger.warning(\n\u001b[1;32m    682\u001b[0m                         \u001b[0;34m\"Each 'words' should be a list of words (usually unicode strings).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "model_dbow.build_vocab([train_tagged, test_tagged, unlabeled_train_tagged])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里遇到一个问题，在构建词典的时候，我们希望把[train_tagged, test_tagged, unlabeled_train_tagged]这三个都用起来，但是不论使用`np.concatenate((train_tagged, test_tagged, unlabeled_train_tagged))`的方法，还是用`[train_tagged, test_tagged, unlabeled_train_tagged]`形式，都不能自动合成一个输入。这里只能曲线救国了，把三个clean_reviews先整合到一起，直接得到一个包含所有reviews的document。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_clean_reviews = np.concatenate((clean_train_reviews, clean_test_reviews, clean_unlabeled_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tagged = tag_reviews(all_clean_reviews, 'ALL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_tagged \n",
    "# test_tagged \n",
    "# unlabeled_train_tagged\n",
    "# all_tagged, contain three parts above\n",
    "len(all_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种方法是，建立一个空list，然后对三个tag对象迭代，全部加到一个list里："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tagged = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_objects = [train_tagged, test_tagged, unlabeled_train_tagged]\n",
    "for tag_object in tag_objects:\n",
    "    for tag in tag_object:\n",
    "        all_tagged.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次尝试构建词典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(min_count=1, window=10, size=100, sample=1e-3, negative=5, dm=0, workers=3, seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dbow.build_vocab(all_tagged, progress_per=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行多次重复训练，每一次都需要对训练数据重新打乱，以提高精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28992769"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练集\n",
    "model_dbow.train(train_tagged, total_examples=len(train_tagged), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yamashita', 0.4659208655357361),\n",
       " ('gombon', 0.4030294716358185),\n",
       " ('choirs', 0.39732831716537476),\n",
       " ('meighan', 0.39478492736816406),\n",
       " ('bekim', 0.393124520778656),\n",
       " ('counsil', 0.3909412622451782),\n",
       " ('findus', 0.38706544041633606),\n",
       " ('rhoades', 0.38631200790405273),\n",
       " ('unwit', 0.3849732279777527),\n",
       " ('corporatization', 0.38302838802337646)]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23571487,  0.45775369, -0.19429441,  0.19739306,  0.06349532,\n",
       "       -0.32546872, -0.29554409,  0.11032848, -0.33108968,  0.47355616,\n",
       "       -0.18420415, -0.12026758,  0.11244816,  0.39083016,  0.07875296,\n",
       "        0.0182258 ,  0.08871382, -0.46112907,  0.06538947, -0.30757257,\n",
       "       -0.03677357,  0.4571391 , -0.11571738, -0.28078708, -0.3350139 ,\n",
       "        0.06919808, -0.11012798,  0.29829952, -0.12432194,  0.3541871 ,\n",
       "       -0.03057882,  0.06124721,  0.10388241,  0.090198  , -0.07303879,\n",
       "       -0.30610693, -0.05933514,  0.01993979,  0.35669476, -0.32851699,\n",
       "       -0.40991127,  0.03481026,  0.29278004,  0.07365297, -0.10998157,\n",
       "        0.10443848, -0.08990712,  0.1885464 ,  0.03642843, -0.2410778 ,\n",
       "        0.41257307,  0.16583849, -0.25326484,  0.09907204, -0.24363291,\n",
       "       -0.72867095, -0.16245642,  0.14579734, -0.19472615, -0.28438318,\n",
       "        0.29534045,  0.43244103, -0.00199401, -0.07133139, -0.14348222,\n",
       "       -0.17259851,  0.08573415,  0.32934815, -0.15625125,  0.32024667,\n",
       "        0.5552054 ,  0.56315023, -0.04874059, -0.3010287 , -0.21359272,\n",
       "       -0.13677195,  0.13343646, -0.11454517,  0.10188863, -0.08642682,\n",
       "        0.0916711 , -0.06502925, -0.24792451,  0.06505245,  0.01901315,\n",
       "       -0.07020592, -0.12434806, -0.46508047,  0.26942134, -0.20356192,\n",
       "       -0.10719759,  0.19499168, -0.31991088,  0.1241444 , -0.03657116,\n",
       "        0.12964152,  0.34537327,  0.04218253, -0.20115213,  0.01226592], dtype=float32)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.docvecs['TRAIN_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 100)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(model_dbow.docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00421057,  0.00187568, -0.00238172, -0.00200227,  0.00264446,\n",
       "        0.00183726,  0.00278547,  0.00327837, -0.0033134 ,  0.0025815 ,\n",
       "       -0.00311388, -0.00161481,  0.00120862, -0.00446639, -0.00444229,\n",
       "        0.00184929,  0.00492307, -0.00024591, -0.00163187, -0.00164549,\n",
       "       -0.00123777, -0.00072104,  0.00135876,  0.00248231,  0.00436443,\n",
       "       -0.00171494,  0.00202779, -0.00377534,  0.00079256, -0.00202123,\n",
       "        0.00443694,  0.00239019,  0.00180055, -0.0011058 , -0.00277819,\n",
       "        0.00379431, -0.00499493, -0.00345632,  0.00391399, -0.00172628,\n",
       "       -0.0010942 ,  0.00473932, -0.00321528, -0.00195504, -0.00295239,\n",
       "       -0.0042989 ,  0.00058964, -0.00414301,  0.00104371, -0.00337555,\n",
       "        0.00157633, -0.00178631, -0.00185223,  0.00257274, -0.00103555,\n",
       "        0.00420367,  0.00315895,  0.00051415,  0.00232465, -0.00388533,\n",
       "       -0.00181878,  0.00057972, -0.00340101, -0.00182394, -0.00126339,\n",
       "        0.00448554, -0.00453214,  0.00489036,  0.00281866, -0.00340575,\n",
       "       -0.0046989 , -0.00466091,  0.00119723,  0.00322923,  0.00249409,\n",
       "       -0.00208731, -0.00269107,  0.00243632, -0.00302893, -0.00374632,\n",
       "       -0.00430649,  0.0049421 , -0.00477623,  0.00483272,  0.00031321,\n",
       "        0.00146674,  0.0035317 , -0.00495722,  0.00169665, -0.0007368 ,\n",
       "       -0.00233712,  0.00070946,  0.00297276, -0.00033736, -0.0046086 ,\n",
       "        0.00302148,  0.00012539,  0.00336738, -0.00101791,  0.00047403], dtype=float32)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.docvecs['TEST_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在能正常用tag得到向量了，可能和我更改了tag_reviews和去掉shuffle有关，但是为什么test也会有向量呢？训练的时候我明明只添加了train_tag啊。我猜测是用于训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_array = []\n",
    "for i in range(len(train_tagged)):\n",
    "    train_array.append(model_dbow.docvecs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_target = train['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['naturally', 'film', 'main', 'themes', 'mortality', 'nostalgia', 'loss', 'innocence', 'perhaps', 'surprising', 'rated', 'highly', 'older', 'viewers', 'younger', 'ones', 'however', 'craftsmanship', 'completeness', 'film', 'anyone', 'enjoy', 'pace', 'steady', 'constant', 'characters', 'full', 'engaging', 'relationships', 'interactions', 'natural', 'showing', 'need', 'floods', 'tears', 'show', 'emotion', 'screams', 'show', 'fear', 'shouting', 'show', 'dispute', 'violence', 'show', 'anger', 'naturally', 'joyce', 'short', 'story', 'lends', 'film', 'ready', 'made', 'structure', 'perfect', 'polished', 'diamond', 'small', 'changes', 'huston', 'makes', 'inclusion', 'poem', 'fit', 'neatly', 'truly', 'masterpiece', 'tact', 'subtlety', 'overwhelming', 'beauty'], tags=['TEST_0'])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tagged[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一种test data\n",
    "关于如何得到test数据，还有很多种尝试方法。这里我们用模型来预测每一个test数据的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_array = []\n",
    "for i in range(len(test_tagged)):\n",
    "    test_array.append(model_dbow.infer_vector(test_tagged[i].words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16741326,  0.55852312, -0.03362535,  0.31930959,  0.08296984,\n",
       "       -0.07947304, -0.07797807,  0.14376642, -0.15254921, -0.2210415 ,\n",
       "       -0.02304331, -0.17412151,  0.34129703,  0.18006454,  0.34570119,\n",
       "       -0.14487725,  0.02051304, -0.57484555, -0.05133546,  0.01037545,\n",
       "        0.22275317,  0.38014448, -0.31867149, -0.06234517, -0.22119807,\n",
       "       -0.07582545, -0.100991  ,  0.35394457,  0.13840154, -0.16435973,\n",
       "       -0.0184453 , -0.13852729,  0.26403272,  0.06933542,  0.05637591,\n",
       "       -0.28032136,  0.12045938,  0.27054679,  0.34062421, -0.40137991,\n",
       "       -0.2334284 ,  0.49347696, -0.32019919, -0.02283208, -0.05977324,\n",
       "       -0.11325342, -0.84192359,  0.13864151, -0.4063195 , -0.1884138 ,\n",
       "        0.10607169,  0.28732526,  0.15027149, -0.28106824, -0.40275922,\n",
       "       -0.46907219, -0.0948663 ,  0.01465809, -0.0377506 , -0.51499683,\n",
       "        0.05009205,  0.36480644,  0.08054388,  0.03245149, -0.08314686,\n",
       "       -0.04861232, -0.21862179,  0.11567195, -0.05528564,  0.06197859,\n",
       "        0.14919399,  0.49989033, -0.19875346, -0.08359645,  0.09578349,\n",
       "       -0.01333919,  0.01459437, -0.42872912,  0.23159017,  0.1078401 ,\n",
       "        0.16176328, -0.24032971, -0.41340208,  0.30498046,  0.3040407 ,\n",
       "       -0.04334924, -0.31853038, -0.38128611,  0.94146234, -0.02279104,\n",
       "       -0.07541022, -0.29128289, -0.76130778, -0.13844892, -0.22762389,\n",
       "        0.06823862,  0.52061337,  0.02713343,  0.00608291, -0.25868705], dtype=float32)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二种test data\n",
    "\n",
    "这一次直接取训练好的模型中，那些tag为test的:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST_0'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tagged[0].tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_array = []\n",
    "for i in range(len(test_tagged)):\n",
    "    tag = test_tagged[i].tags[0]\n",
    "    test_array.append(model_dbow.docvecs[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00421057,  0.00187568, -0.00238172, -0.00200227,  0.00264446,\n",
       "        0.00183726,  0.00278547,  0.00327837, -0.0033134 ,  0.0025815 ,\n",
       "       -0.00311388, -0.00161481,  0.00120862, -0.00446639, -0.00444229,\n",
       "        0.00184929,  0.00492307, -0.00024591, -0.00163187, -0.00164549,\n",
       "       -0.00123777, -0.00072104,  0.00135876,  0.00248231,  0.00436443,\n",
       "       -0.00171494,  0.00202779, -0.00377534,  0.00079256, -0.00202123,\n",
       "        0.00443694,  0.00239019,  0.00180055, -0.0011058 , -0.00277819,\n",
       "        0.00379431, -0.00499493, -0.00345632,  0.00391399, -0.00172628,\n",
       "       -0.0010942 ,  0.00473932, -0.00321528, -0.00195504, -0.00295239,\n",
       "       -0.0042989 ,  0.00058964, -0.00414301,  0.00104371, -0.00337555,\n",
       "        0.00157633, -0.00178631, -0.00185223,  0.00257274, -0.00103555,\n",
       "        0.00420367,  0.00315895,  0.00051415,  0.00232465, -0.00388533,\n",
       "       -0.00181878,  0.00057972, -0.00340101, -0.00182394, -0.00126339,\n",
       "        0.00448554, -0.00453214,  0.00489036,  0.00281866, -0.00340575,\n",
       "       -0.0046989 , -0.00466091,  0.00119723,  0.00322923,  0.00249409,\n",
       "       -0.00208731, -0.00269107,  0.00243632, -0.00302893, -0.00374632,\n",
       "       -0.00430649,  0.0049421 , -0.00477623,  0.00483272,  0.00031321,\n",
       "        0.00146674,  0.0035317 , -0.00495722,  0.00169665, -0.0007368 ,\n",
       "       -0.00233712,  0.00070946,  0.00297276, -0.00033736, -0.0046086 ,\n",
       "        0.00302148,  0.00012539,  0.00336738, -0.00101791,  0.00047403], dtype=float32)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_array = np.array(train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_array, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = lr.predict(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output...\n"
     ]
    }
   ],
   "source": [
    "print(\"output...\")\n",
    "output = pd.DataFrame(data={'id': test['id'], 'sentiment': result})\n",
    "output.to_csv('doc2vec2.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一种test data提交的效果是0.81。下面尝试第二种的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result2 = lr.predict(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output...\n"
     ]
    }
   ],
   "source": [
    "print(\"output...\")\n",
    "output = pd.DataFrame(data={'id': test['id'], 'sentiment': result2})\n",
    "output.to_csv('doc2vec3.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二种的结果是0.5，很惨，看来test数据不应该直接从模型里拿出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 增加训练数据\n",
    "\n",
    "上面训练doc2vec的时候，我们只用了train_tagged，这一次使用train_tagged和unlabeled_train_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tagged2 = []\n",
    "tag_objects = [train_tagged, unlabeled_train_tagged]\n",
    "for tag_object in tag_objects:\n",
    "    for tag in tag_object:\n",
    "        train_tagged2.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(min_count=1, window=10, size=100, sample=1e-3, negative=5, dm=0, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dbow.build_vocab(all_tagged, progress_per=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87255268"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.train(train_tagged2, total_examples=len(train_tagged2), epochs=10, start_alpha=0.025, end_alpha=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TRAIN_0']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged[0].tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_array = []\n",
    "for i in range(len(train_tagged)):\n",
    "    tag = train_tagged[i].tags[0]\n",
    "    train_array.append(model_dbow.docvecs[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_array = np.array(train_array)\n",
    "train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_array = []\n",
    "for i in range(len(test_tagged)):\n",
    "    test_array.append(model_dbow.infer_vector(test_tagged[i].words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_array, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = lr.predict(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output...\n"
     ]
    }
   ],
   "source": [
    "print(\"output...\")\n",
    "output = pd.DataFrame(data={'id': test['id'], 'sentiment': result})\n",
    "output.to_csv('doc2vec4.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果是0.855，还可以，起码比之前的bag of words要高了。下面我换成随机森林试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "forest = forest.fit( train_array, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = forest.predict(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output...\n"
     ]
    }
   ],
   "source": [
    "print(\"output...\")\n",
    "output = pd.DataFrame(data={'id': test['id'], 'sentiment': result})\n",
    "output.to_csv('doc2vec5.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得分是0.775……差的好多"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型调参\n",
    "\n",
    "这一次尝试在多次训练迭代的时候，shuffle数据。下一次尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(min_count=1, window=10, size=100, sample=1e-3, negative=5, dm=0, workers=3)\n",
    "model_dbow.build_vocab(all_tagged, progress_per=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    shuffle(train_tagged2)\n",
    "    model_dbow.train(train_tagged2, total_examples=len(train_tagged2), epochs=1, start_alpha=0.025, end_alpha=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2390765 ,  0.1142169 ,  0.20712513,  0.89075464, -0.22244544,\n",
       "        0.27381375, -0.17348099, -0.00806451, -0.07966816, -0.25908321,\n",
       "        0.14005575,  0.81300563,  0.42594698, -0.41442031, -0.50202197,\n",
       "       -0.3608807 , -0.18744084, -0.74130625, -0.3690764 ,  0.73912692,\n",
       "        0.488134  ,  0.57566327,  0.03431462, -0.11286174,  0.09621059,\n",
       "       -0.53745824,  0.19251792,  0.03610734, -0.16454478,  0.46170405,\n",
       "        0.32712093,  0.35736078, -0.18140447,  0.53149986,  0.71819407,\n",
       "       -0.05336506, -0.17479719, -0.09747268,  0.21144409,  0.26047847,\n",
       "       -0.12783214, -0.17676389, -0.22317085,  0.25071913,  0.287783  ,\n",
       "        0.58708721, -0.13033538,  0.02738427, -0.17963417, -0.46462777,\n",
       "        0.12726952, -0.59729439, -0.03004212, -0.04822551,  0.11562251,\n",
       "        0.53002852, -0.0609221 ,  0.52630454,  0.11480941, -1.12891674,\n",
       "        0.2790131 , -0.22757432,  0.42739171, -0.02386028,  0.0052669 ,\n",
       "        0.58239245,  0.04886733,  0.25173002,  0.53091973, -0.30668002,\n",
       "       -0.65927392,  0.15028897, -0.12135617, -0.48292008, -0.27421358,\n",
       "       -0.52075577,  0.43338963, -0.13219024, -0.16607863,  0.61988735,\n",
       "       -0.72212499,  0.14273369,  0.5413444 ,  0.09514584, -1.01347816,\n",
       "        0.08230074, -0.42629448, -0.12425663,  0.43998876, -0.55400598,\n",
       "       -0.06349921,  0.25537333, -0.07176838,  0.19265765, -0.09648473,\n",
       "       -0.03791967, -0.28810632, -0.37075368,  0.15192722, -0.00719192], dtype=float32)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.docvecs['TRAIN_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2390765 ,  0.1142169 ,  0.20712513,  0.89075464, -0.22244544,\n",
       "        0.27381375, -0.17348099, -0.00806451, -0.07966816, -0.25908321,\n",
       "        0.14005575,  0.81300563,  0.42594698, -0.41442031, -0.50202197,\n",
       "       -0.3608807 , -0.18744084, -0.74130625, -0.3690764 ,  0.73912692,\n",
       "        0.488134  ,  0.57566327,  0.03431462, -0.11286174,  0.09621059,\n",
       "       -0.53745824,  0.19251792,  0.03610734, -0.16454478,  0.46170405,\n",
       "        0.32712093,  0.35736078, -0.18140447,  0.53149986,  0.71819407,\n",
       "       -0.05336506, -0.17479719, -0.09747268,  0.21144409,  0.26047847,\n",
       "       -0.12783214, -0.17676389, -0.22317085,  0.25071913,  0.287783  ,\n",
       "        0.58708721, -0.13033538,  0.02738427, -0.17963417, -0.46462777,\n",
       "        0.12726952, -0.59729439, -0.03004212, -0.04822551,  0.11562251,\n",
       "        0.53002852, -0.0609221 ,  0.52630454,  0.11480941, -1.12891674,\n",
       "        0.2790131 , -0.22757432,  0.42739171, -0.02386028,  0.0052669 ,\n",
       "        0.58239245,  0.04886733,  0.25173002,  0.53091973, -0.30668002,\n",
       "       -0.65927392,  0.15028897, -0.12135617, -0.48292008, -0.27421358,\n",
       "       -0.52075577,  0.43338963, -0.13219024, -0.16607863,  0.61988735,\n",
       "       -0.72212499,  0.14273369,  0.5413444 ,  0.09514584, -1.01347816,\n",
       "        0.08230074, -0.42629448, -0.12425663,  0.43998876, -0.55400598,\n",
       "       -0.06349921,  0.25537333, -0.07176838,  0.19265765, -0.09648473,\n",
       "       -0.03791967, -0.28810632, -0.37075368,  0.15192722, -0.00719192], dtype=float32)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.docvecs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现上面还是按顺序来的，感觉就像没有shuffle过一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_array = []\n",
    "for i in range(len(train_tagged)):\n",
    "    tag = train_tagged[i].tags[0]\n",
    "    train_array.append(model_dbow.docvecs[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_array = []\n",
    "for i in range(len(test_tagged)):\n",
    "    test_array.append(model_dbow.infer_vector(test_tagged[i].words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_array, train_target)\n",
    "result = lr.predict(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output...\n"
     ]
    }
   ],
   "source": [
    "print(\"output...\")\n",
    "output = pd.DataFrame(data={'id': test['id'], 'sentiment': result})\n",
    "output.to_csv('doc2vec6.csv', index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果是0.87，接近BOW_chi_tfidf.csv的0.88了。好了，我就用这个做deep模型吧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过运行程序得到了dbow和dm两个模型，前者还是0.87，后者0.75。果然dbow效果好"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
